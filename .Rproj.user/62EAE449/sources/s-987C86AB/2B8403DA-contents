
#==============================================================
rm(list = ls())  # Limpia el ambiente
graphics.off()   # Limpia los graficos
gc(reset = TRUE) # Recoge la basura de la RAM
cat("\f")        # Limpia la consola
#==============================================================


## Librerías / Configuraciones Ini
library(dplyr)
library(data.table)
library(ggplot2)
library(lubridate)
library(factoextra)
library(corrplot)
library(FactoMineR)
library(knitr)

# cambio de directorio
dir <- getwd()
setwd(dir)

# Lectura de datos
# df <- fread("owid-covid-data.csv")
df <- fread("https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/owid-covid-data.csv")
head(View(df))
 

# Explaración datos crudos ------------------------------------------------
sort(unique(df$continent))
sort(unique(df$location))   # 215 paises en la base original
sort(unique(df$iso_code))   # este campo puede ser útil para las gráficas
sort(unique(df$date))       # desde 202001 hasta 202104


# Diccionario de datos
dicc <- read.csv(url("https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/owid-covid-codebook.csv"))

# Cambio los nombre de las variables
dicc$column <- gsub("per_million","pm",dicc$column)
dicc$column <- gsub("per_thousand","pt",dicc$column)
dicc$column <- gsub("per_hundred","ph",dicc$column)

dicc <- dicc %>% select(-source)



# Filtramos los datos de américa del sur
df_south <- df %>%  filter(continent == "South America")


 # Calidad de datos ------------------------------------------------------
# Queremos observar la cantidad de datos faltantes por variable
dim1 <- dim(df_south)[1] 
na_printer <- function(columna,dim1,df){
  
    messg <- paste("Variable: " , columna)
    na_pr <- round(sum(is.na(df[[columna]])) / dim1 * 100, 2)
    na_pr_mss <- paste(na_pr,"%")
    mssg_na <- paste("Total de datos faltantes", na_pr_mss, sep = " ")
    
    print(messg)
    print(mssg_na)

    return(c(columna,na_pr))
}

columnas <- names(df)
df_total <- data.frame()
new_cols <- c()

for (col in columnas){
  valores <- na_printer(col,dim1,df_south)
  df_val <- data.frame(Variable = valores[1], Pr_Na = valores[2])
  df_total <- rbind(df_total,df_val)

    if ( as.numeric(valores[2]) < 10){
    new_cols <- c(new_cols,col)
  }
 
}

df_south_select <- df_south %>% select(all_of(new_cols))

america <- c("North America","South America")
df_total_select_ame <- df %>% select(all_of(new_cols)) %>%
  filter(continent %in% america)


# Agrupamientos -----------------------------------------------------------
## SELECCION DE VARIABLES PARA LA MEDIA
# ELIMINACIÓN SMOOT Y TEST UNITS
columns_s <- names(df_south_select)
columns_smot <- columns_s[grepl("smoothed",columns_s)]
columns_not_select <- c("test_units",columns_smot)
columns_to_select <- columns_s[! columns_s  %in% columns_not_select ]

# AGRUPAMIENTO
agrupamiento_por_media <- function(df_filter){
  df_south_agrp <- df_filter %>% 
    select( all_of(columns_to_select)) %>%
    mutate(Mes = month(date),
           Yr = year(date)) %>% 
    group_by(location,Mes,Yr) %>% 
    summarise_if(is.numeric, ~mean(.x,na.rm = TRUE))
  
  df_total_mean <- df_south_agrp %>% 
    group_by(location)%>% 
    summarise_if(is.numeric, ~mean(.x,na.rm = TRUE))
  
  df_total_mean <- na.omit(df_total_mean) %>% 
    select(-"Mes",-"Yr")
  
  return(df_total_mean)
}

df_total_mean_sur <- agrupamiento_por_media(df_south_select)
df_total_mean_america <- agrupamiento_por_media(df_total_select_ame)

# Análsis Gráfico ---------------------------------------------------------

cor_plots_vars <- function(df_total_mean,limite_inf,limite_sup){
  
  cols_names_num<- names(df_total_mean)[limite_inf:limite_sup]
  
  c2<-cor(df_total_mean[cols_names_num])
  print(corrplot(c2,method = "number",number.cex = .8,tl.cex = 0.8,cl.cex = 0.8,tl.col = "gray2"))
  
}

pca_plots <- function(df_total_mean){
  
  df_total_pca <- df_total_mean
  ### PRCOMP
  mt_pca <- as.matrix(df_total_pca[, -1])
  rownames(mt_pca) <- df_total_pca[,1][[1]]
  res.pca <- prcomp(mt_pca,  scale = TRUE)
  # fviz_pca_var(res.pca, col.var = "contrib",
  #              gradient.cols = c("white", "blue", "red"),
  #              ggtheme = theme_minimal())
  
  print(fviz_pca_ind(res.pca, col.ind = "cos2", 
               gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
               repel = TRUE))
  
  print(fviz_pca_biplot(res.pca,gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07")))
  
  
  # df_total$Pr_Na <- as.numeric(df_total$Pr_Na)
  # datos = data.frame(x = c(1, 2, NA, 3), y = c(NA, NA, 4, 5))
  # sum(is.na(datos))/prod(dim(datos))
  
  return(res.pca)
}
## Correlacion Paises Sur-America
cor_plots_vars(df_total_mean_sur,5,10)
## Correlacion Paises America
cor_plots_vars(df_total_mean_america,10,18)
## PCA SUR-AMERICA
pca_sur <- pca_plots(df_total_mean_sur)
pca_ame <- pca_plots(df_total_mean_america)

#==========================================================================
# cambio el nombre de las variables 

cambio_variables <- function(data_frame){
  
  new_nombres = gsub("per_million","pm", names(data_frame))
  new_nombres = gsub("per_thousand","pt", new_nombres)
  colnames(data_frame) <- c(new_nombres)
  return(data_frame)  
}

df_total_mean_america <- cambio_variables(df_total_mean_america)

#==========================================================================
# si las variables son demasiado correlacionas, quitamos una de ellas
# 1 nos interesa el conteo de casos y muertes por millon de habitantes

datos_america <- df_total_mean_america %>% 
                 select(-c("total_cases","new_cases","total_deaths","new_deaths"))

cor_plots_vars(datos_america,2,17)

# los nuevos casos y los casos total son demasiado correlacionados
datos_america <- datos_america %>% 
  select(-c("total_cases_pm","total_deaths_pm"))

# la edad media está muy correlaciona con 
# las variables de mayores de 65 y 70
datos_america <- datos_america %>% 
  select(-c("aged_65_older","aged_70_older"))


cor_plots_vars(datos_america,2,13)
pca_ame <- pca_plots(datos_america)
