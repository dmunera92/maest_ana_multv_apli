---
title: "PCA Covid19 América"
author: "Daniel Munera y Cristian Londoño"
date: "04/04/2021"
output:
  html_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## PCA para datos de Covid19 en el continente americano

En el presente trabajo se analizarán las principales variables relacionadas con la propagación del Covid-19 en el continente americano, considerando países tanto de Sur-América, Centro América y Norte América. Para el análisis se utilizará la técnica de análisis de componentes principales(PCA) y representaciones Biplot.

```{r, echo=FALSE, message=FALSE}

# cargamos librerias
library(dplyr)
library(data.table)
library(ggplot2)
library(lubridate)
library(factoextra)
library(corrplot)
library(FactoMineR)
library(knitr)


```


```{r, echo=FALSE}
# cambio de directorio
dir <- getwd()
setwd(dir)


```


```{r, echo=FALSE, message=FALSE}
# Lectura de datos
df <- fread("https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/owid-covid-data.csv")
dicc <- read.csv(url("https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/owid-covid-codebook.csv"))
#head(View(df))
```

Para el análisis se utilizarán datos de la publicación de la Universidad de Oxford  OurWorldInData actualizados hasta el 03 de abril del 2021. La descripción de las variables a utilizar se presentan a continuación:

```{r, echo=FALSE}
# Cambio los nombre de las variables
dicc$column <- gsub("per_million","pm",dicc$column)
dicc$column <- gsub("per_thousand","pt",dicc$column)
dicc$column <- gsub("per_hundred","ph",dicc$column)

dicc <- dicc %>% select(-source)

# columnas a usar:
 variables <- c("location","new_cases_pm", "new_deaths_pm" , "stringency_index", "population",        "population_density","median_age", "gdp_per_capita", "cardiovasc_death_rate", "diabetes_prevalence","hospital_beds_pt", "life_expectancy","human_development_index")
 
dicc <- dicc %>% filter(column %in% dicc)

# cambio del nombre de las variables
colnames(dicc) <- c("Variable","Descripción")

kable(dicc)

```





```{r, echo=FALSE}
# tomamos inicialmente solo datos de suramerica
df_south <- df %>%  filter(continent == "South America")

```

```{r, echo=FALSE}
# Calidad de datos ------------------------------------------------------
# Queremos observar la cantidad de datos faltantes por variable
dim1 <- dim(df_south)[1] 

na_printer <- function(x,dim1,df){
  
    messg <- paste("Variable: " , x)
    na_pr <- round(sum(is.na(df[[x]])) / dim1 * 100, 2)
    na_pr_mss <- paste(na_pr,"%")
    mssg_na <- paste("Total de datos faltantes", na_pr_mss, sep = " ")
    
    # print(messg)
    # print(mssg_na)

    return(c(x,na_pr))
}


columnas <- names(df)
df_total <- data.frame()
new_cols <- c()

for (col in columnas){
  valores <- na_printer(col,dim1,df_south)
  df_val <- data.frame(Variable = valores[1], Pr_Na = valores[2])
  df_total <- rbind(df_total,df_val)

    if ( as.numeric(valores[2]) < 10){
    new_cols <- c(new_cols,col)
  }
 
}

df_south_select <- df_south %>% select(all_of(new_cols))

america <- c("North America","South America")
df_total_select_ame <- df %>% select(all_of(new_cols)) %>%
  filter(continent %in% america)

# Agrupamientos -----------------------------------------------------------
## SELECCION DE VARIABLES PARA LA MEDIA
# ELIMINACIÓN SMOOT Y TEST UNITS
columns_s <- names(df_south_select)
columns_smot <- columns_s[grepl("smoothed",columns_s)]
columns_not_select <- c("test_units",columns_smot)
columns_to_select <- columns_s[! columns_s  %in% columns_not_select ]

# AGRUPAMIENTO
agrupamiento_por_media <- function(df_filter){
  df_south_agrp <- df_filter %>% 
    select( all_of(columns_to_select)) %>%
    mutate(Mes = month(date),
           Yr = year(date)) %>% 
    group_by(location,Mes,Yr) %>% 
    summarise_if(is.numeric, ~mean(.x,na.rm = TRUE))
  
  df_total_mean <- df_south_agrp %>% 
    group_by(location)%>% 
    summarise_if(is.numeric, ~mean(.x,na.rm = TRUE))
  
  df_total_mean <- na.omit(df_total_mean) %>% 
    select(-"Mes",-"Yr")
  
  return(df_total_mean)
}

df_total_mean_sur <- agrupamiento_por_media(df_south_select)
df_total_mean_america <- agrupamiento_por_media(df_total_select_ame)

cor_plots_vars <- function(df_total_mean,limite_inf,limite_sup){
  
  cols_names_num<- names(df_total_mean)[limite_inf:limite_sup]
  
  c2<-cor(df_total_mean[cols_names_num])
  print(corrplot(c2,method = "number",number.cex = .8,tl.cex = 0.8,cl.cex = 0.8,tl.col = "gray2"))
  
}

cor_plots_vars(df_total_mean_sur,5,10)

## Correlacion Paises America
cor_plots_vars(df_total_mean_america,10,18)


pca_plots <- function(df_total_mean){
  
  df_total_pca <- df_total_mean
  ### PRCOMP
  mt_pca <- as.matrix(df_total_pca[, -1])
  rownames(mt_pca) <- df_total_pca[,1][[1]]
  res.pca <- prcomp(mt_pca,  scale = TRUE)
  # fviz_pca_var(res.pca, col.var = "contrib",
  #              gradient.cols = c("white", "blue", "red"),
  #              ggtheme = theme_minimal())
  
  print(fviz_pca_ind(res.pca, col.ind = "cos2", 
               gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
               repel = TRUE))
  
  print(fviz_pca_biplot(res.pca,gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07")))
  
  
  # df_total$Pr_Na <- as.numeric(df_total$Pr_Na)
  # datos = data.frame(x = c(1, 2, NA, 3), y = c(NA, NA, 4, 5))
  # sum(is.na(datos))/prod(dim(datos))
  
  return(res.pca)
}



```



```{r, fig.width=12, fig.height=8}
## PCA SUR-AMERICA
pca_sur <- pca_plots(df_total_mean_sur)
plot(pca_sur)

pca_ame <- pca_plots(df_total_mean_america)
plot(pca_ame)

```

